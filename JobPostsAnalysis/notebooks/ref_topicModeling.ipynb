{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('jobpostsanalysis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a134eccee76ee2a5312d6730b4c256413df435750bc9b712f8f2978337cad195"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from config import CONFIG\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(CONFIG.src_path / 'data' / 'sequential'))\n",
    "\n",
    "from proxyManager import ProxyManager\n",
    "\n"
   ]
  },
  {
   "source": [
    "folders = (CONFIG.data_path / 'external' / 'nipstxt').glob('nips*')\n",
    "papers = []\n",
    "\n",
    "for folder in folders:\n",
    "    files = folder.glob('*')\n",
    "    for file in files:\n",
    "        with open(str(file), encoding='utf8', errors='ignore', mode='r+') as f:\n",
    "            data = f.read()\n",
    "        papers.append(data)\n",
    "\n",
    "len(papers)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1740"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "source": [
    "print(papers[0][:1000])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 \nCONNECTIVITY VERSUS ENTROPY \nYaser S. Abu-Mostafa \nCalifornia Institute of Technology \nPasadena, CA 91125 \nABSTRACT \nHow does the connectivity of a neural network (number of synapses per \nneuron) relate to the complexity of the problems it can handle (measured by \nthe entropy)? Switching theory would suggest no relation at all, since all Boolean \nfunctions can be implemented using a circuit with very low connectivity (e.g., \nusing two-input NAND gates). However, for a network that learns a problem \nfrom examples using a local learning rule, we prove that the entropy of the \nproblem becomes a lower bound for the connectivity of the network. \nINTRODUCTION \nThe most distinguishing feature of neural networks is their ability to spon- \ntaneously learn the desired function from 'training' samples, i.e., their ability \nto program themselves. Clearly, a given neural network cannot just learn any \nfunction, there must be some restrictions on which networks can learn which \nfunctions. One obv\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## Basic Text Wrangling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/tnguyen2921/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "1740\n",
      "CPU times: user 32.6 s, sys: 37.7 ms, total: 32.7 s\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# def normalize_corpus(papers):\n",
    "#     norm_papers = []\n",
    "#     for paper in papers:\n",
    "#         paper = paper.lower()\n",
    "#         paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
    "#         paper_tokens = [wln.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "#         paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "#         paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "#         paper_tokens = list(filter(None, paper_tokens))\n",
    "#         if paper_tokens:\n",
    "#             norm_papers.append(papers)\n",
    "#     return norm_papers\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "    return norm_papers\n",
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['connectivity',\n",
       " 'versus',\n",
       " 'entropy',\n",
       " 'yaser',\n",
       " 'abu',\n",
       " 'mostafa',\n",
       " 'california',\n",
       " 'institute',\n",
       " 'technology',\n",
       " 'pasadena']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "norm_papers[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['connectivity',\n",
       " 'versus',\n",
       " 'entropy',\n",
       " 'yaser',\n",
       " 'abu_mostafa',\n",
       " 'california_institute',\n",
       " 'technology_pasadena',\n",
       " 'ca_abstract',\n",
       " 'doe',\n",
       " 'connectivity']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "bigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20, delimiter=b\"_\")\n",
    "\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "bigram_model[norm_papers[0]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample word to number mappings: [(0, '0a'), (1, '2h'), (2, '2h2'), (3, '2he'), (4, '2n'), (5, '__c'), (6, '_c'), (7, '_k'), (8, 'a2'), (9, 'ability'), (10, 'abu_mostafa'), (11, 'access'), (12, 'accommodate'), (13, 'according'), (14, 'accumulated')]\nTotal Vocabulary Size: 78892\n"
     ]
    }
   ],
   "source": [
    "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\n",
    "print(\"Sample word to number mappings:\", list(dictionary.items())[:15])\n",
    "print(\"Total Vocabulary Size:\", len(dictionary))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Vocabulary Size: 7756\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "print(\"Total Vocabulary Size:\", len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(3, 1), (12, 3), (14, 1), (15, 1), (16, 1), (17, 16), (20, 1), (24, 1), (26, 1), (31, 3), (35, 1), (36, 1), (40, 3), (41, 5), (42, 1), (48, 1), (53, 3), (55, 1), (56, 2), (58, 1), (60, 3), (63, 5), (64, 4), (65, 2), (73, 1), (74, 1), (75, 1), (76, 1), (77, 3), (82, 1)]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n",
    "print(bow_corpus[1][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('ability', 1), ('aip', 3), ('although', 1), ('american_institute', 1), ('amount', 1), ('analog', 16), ('appears', 1), ('architecture', 1), ('aspect', 1), ('available', 3), ('become', 1), ('becomes', 1), ('binary', 3), ('biological', 5), ('bit', 1), ('cannot', 1), ('circuit', 3), ('collective', 1), ('compare', 2), ('complex', 1), ('computing', 3), ('conference', 5), ('connected', 4), ('connectivity', 2), ('define', 1), ('defined', 1), ('defines', 1), ('definition', 1), ('denker', 3), ('designed', 1), ('desired', 4), ('diagonal', 1), ('difference', 1), ('directly', 2), ('ed', 1), ('el', 2), ('element', 3), ('equivalent', 1), ('eventually', 1), ('feature', 2), ('final', 4), ('find', 2), ('fixed', 2), ('frequency', 1), ('furthermore', 1), ('generating', 1), ('get', 1), ('global', 6), ('go', 1), ('hence', 1)]\n"
     ]
    }
   ],
   "source": [
    "print([(dictionary[idx] , freq) for idx, freq in bow_corpus[1][:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of papers: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Total number of papers:', len(bow_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "TOTAL_TOPICS = 10\n",
    "\n",
    "lsi_bow = gensim.models.LsiModel(bow_corpus, id2word=dictionay, num_topics=TOTAL_TOPICS, onepass=True, chunksize=1740, power_iters=1000)\n",
    "\n"
   ]
  },
  {
   "source": [
    "### LDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 8min 20s, sys: 19.2 s, total: 8min 39s\nWall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TOTAL_TOPICS = 10\n",
    "lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740, alpha=\"auto\", eta=\"auto\", random_state=42, iterations=500, num_topics=TOTAL_TOPICS, passes=20, eval_every=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic #1:\n0.013*\"circuit\" + 0.012*\"chip\" + 0.008*\"neuron\" + 0.008*\"analog\" + 0.007*\"current\" + 0.007*\"bit\" + 0.006*\"voltage\" + 0.005*\"node\" + 0.005*\"word\" + 0.005*\"vector\" + 0.005*\"processor\" + 0.004*\"implementation\" + 0.004*\"threshold\" + 0.004*\"computation\" + 0.004*\"element\" + 0.004*\"signal\" + 0.004*\"pattern\" + 0.004*\"design\" + 0.004*\"memory\" + 0.004*\"parallel\"\n\nTopic #2:\n0.030*\"image\" + 0.012*\"object\" + 0.011*\"feature\" + 0.006*\"pixel\" + 0.006*\"visual\" + 0.005*\"representation\" + 0.005*\"recognition\" + 0.005*\"unit\" + 0.005*\"motion\" + 0.005*\"face\" + 0.005*\"task\" + 0.004*\"view\" + 0.004*\"layer\" + 0.004*\"human\" + 0.004*\"training\" + 0.004*\"position\" + 0.004*\"location\" + 0.004*\"region\" + 0.004*\"character\" + 0.003*\"vector\"\n\nTopic #3:\n0.020*\"neuron\" + 0.017*\"cell\" + 0.012*\"response\" + 0.010*\"stimulus\" + 0.007*\"spike\" + 0.007*\"signal\" + 0.006*\"activity\" + 0.006*\"synaptic\" + 0.005*\"firing\" + 0.005*\"frequency\" + 0.005*\"pattern\" + 0.004*\"current\" + 0.004*\"effect\" + 0.004*\"neural\" + 0.004*\"change\" + 0.004*\"et_al\" + 0.004*\"channel\" + 0.004*\"synapsis\" + 0.003*\"motion\" + 0.003*\"unit\"\n\nTopic #4:\n0.013*\"neuron\" + 0.009*\"cell\" + 0.009*\"pattern\" + 0.008*\"activity\" + 0.007*\"map\" + 0.006*\"unit\" + 0.006*\"dynamic\" + 0.006*\"visual\" + 0.005*\"layer\" + 0.005*\"receptive_field\" + 0.005*\"orientation\" + 0.005*\"connection\" + 0.005*\"cortical\" + 0.005*\"correlation\" + 0.004*\"response\" + 0.004*\"feature\" + 0.004*\"cortex\" + 0.004*\"stimulus\" + 0.004*\"phase\" + 0.004*\"neural\"\n\nTopic #5:\n0.020*\"unit\" + 0.011*\"state\" + 0.010*\"training\" + 0.008*\"rule\" + 0.007*\"net\" + 0.006*\"word\" + 0.006*\"pattern\" + 0.006*\"sequence\" + 0.006*\"node\" + 0.006*\"layer\" + 0.006*\"hidden_unit\" + 0.005*\"activation\" + 0.005*\"architecture\" + 0.005*\"recurrent\" + 0.004*\"recognition\" + 0.004*\"task\" + 0.004*\"vector\" + 0.004*\"trained\" + 0.004*\"context\" + 0.004*\"connection\"\n\nTopic #6:\n0.017*\"signal\" + 0.013*\"memory\" + 0.010*\"noise\" + 0.009*\"control\" + 0.008*\"trajectory\" + 0.007*\"dynamic\" + 0.007*\"state\" + 0.006*\"movement\" + 0.005*\"motor\" + 0.005*\"mapping\" + 0.004*\"pattern\" + 0.004*\"feedback\" + 0.004*\"capacity\" + 0.004*\"position\" + 0.004*\"speech\" + 0.004*\"training\" + 0.004*\"target\" + 0.004*\"arm\" + 0.004*\"vector\" + 0.004*\"change\"\n\nTopic #7:\n0.007*\"vector\" + 0.006*\"equation\" + 0.005*\"let\" + 0.005*\"linear\" + 0.005*\"distribution\" + 0.005*\"approximation\" + 0.005*\"matrix\" + 0.004*\"theorem\" + 0.004*\"convergence\" + 0.004*\"bound\" + 0.004*\"class\" + 0.004*\"training\" + 0.004*\"optimal\" + 0.004*\"theory\" + 0.004*\"consider\" + 0.004*\"solution\" + 0.004*\"probability\" + 0.004*\"estimate\" + 0.004*\"noise\" + 0.003*\"rate\"\n\nTopic #8:\n0.010*\"distribution\" + 0.009*\"probability\" + 0.009*\"variable\" + 0.008*\"mixture\" + 0.006*\"gaussian\" + 0.006*\"tree\" + 0.006*\"prior\" + 0.006*\"structure\" + 0.006*\"component\" + 0.006*\"node\" + 0.005*\"density\" + 0.005*\"class\" + 0.004*\"likelihood\" + 0.004*\"bayesian\" + 0.004*\"estimate\" + 0.004*\"sample\" + 0.004*\"step\" + 0.004*\"log\" + 0.004*\"source\" + 0.003*\"approximation\"\n\nTopic #9:\n0.017*\"training\" + 0.010*\"classifier\" + 0.008*\"classification\" + 0.008*\"class\" + 0.006*\"pattern\" + 0.006*\"feature\" + 0.006*\"test\" + 0.006*\"training_set\" + 0.005*\"vector\" + 0.005*\"prediction\" + 0.004*\"kernel\" + 0.004*\"experiment\" + 0.004*\"trained\" + 0.004*\"linear\" + 0.003*\"technique\" + 0.003*\"rbf\" + 0.003*\"task\" + 0.003*\"size\" + 0.003*\"table\" + 0.003*\"sample\"\n\nTopic #10:\n0.026*\"state\" + 0.013*\"action\" + 0.012*\"control\" + 0.008*\"policy\" + 0.007*\"task\" + 0.007*\"step\" + 0.006*\"reinforcement_learning\" + 0.006*\"controller\" + 0.006*\"environment\" + 0.005*\"optimal\" + 0.005*\"robot\" + 0.004*\"goal\" + 0.004*\"reward\" + 0.003*\"agent\" + 0.003*\"td\" + 0.003*\"current\" + 0.003*\"trial\" + 0.003*\"cost\" + 0.003*\"rate\" + 0.003*\"reinforcement\"\n\n"
     ]
    }
   ],
   "source": [
    "for topic_id, topic in lda_model.print_topics(num_topics=10, num_words=20):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Avg. Coherence Score (Cv): 0.4930044902277785\nAvg. Coherence Score (UMass): -0.9858031202745918\nModel Perplexity: -7.787864315295923\n"
     ]
    }
   ],
   "source": [
    "cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, texts=norm_corpus_bigrams, dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n",
    "\n",
    "umass_coherernce_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, texts=norm_corpus_bigrams, dictionary=dictionary, coherence='u_mass')\n",
    "\n",
    "avg_coherence_umass = umass_coherernce_model_lda.get_coherence()\n",
    "\n",
    "perplexity = lda_model.log_perplexity(bow_corpus)\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PosixPath('/mnt/d/personal_projects/JobPostsAnalysis/JobPostsAnalysis/models/mallet-2.0.8/bin/mallet')"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "MALLET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def topic_model_coherernce_generator(corpus, texts, dictionary, start_topic_count=2, end_topic_count=10, step=1, cpus=1):\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for topic_nums in tqdm(range(start_topic_count, end_topic_count+1, step)):\n",
    "        lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, chunksize=1740, alpha=\"auto\", eta=\"auto\", random_state=42, iterations=500, num_topics=topic_nums, passes=20, eval_every=None)\n",
    "        cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = cv_coherence_model_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_score)\n",
    "        models.append(lda_model)\n",
    "    return models, coherence_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 29/29 [1:12:39<00:00, 150.33s/it]\n"
     ]
    }
   ],
   "source": [
    "lda_models, coherence_scores = topic_model_coherernce_generator(corpus=bow_corpus, texts=norm_corpus_bigrams, dictionary=dictionary, start_topic_count=2, end_topic_count=30, step=1, cpus=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Avg. Coherence Score: -0.9858031202745918\n"
     ]
    }
   ],
   "source": [
    "topics_coherences = lda_model.top_topics(bow_corpus, topn=20)\n",
    "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
    "\n",
    "print(\"Avg. Coherence Score:\", avg_coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_with_wts = [item[0] for item in topics_coherences]\n",
    "\n",
    "print(\"LDA topics with weights\")\n",
    "print(\"=\"*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print(\"Topic #\" + str(idx + 1) + \":\")\n",
    "    print([(term, round(wt, 3)) for wt, term in topic])"
   ]
  }
 ]
}