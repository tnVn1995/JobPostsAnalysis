
import nltk
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import NMF
import text_normalize as tn
import matplotlib.pyplot as plt
%matplotlib inline

jobInfo = pd.read_csv('data/IndeedJobPosts.csv')
jobDesc = pd.read_csv('data/JobDesc.csv')

# Job info
jobInfo.summarylinks = 'https://www.indeed.com' + jobInfo['summarylinks']
display(jobInfo.head(3))

# Merge dataframes
mergedDf = jobInfo.merge(jobDesc, left_on='summarylinks', right_on='links')
mergedDf.head(2)

import re
locs = mergedDf.location.unique()
states = {'TX':[], 'NY':[],'PA':[], 'CA':[], 'MA':[],'DC':[]}
for state in states:                        
    pattern = re.compile(state)
    for idx, x in enumerate(locs):
        if re.search(pattern, x):
            states[state].append(idx)

NY = mergedDf.iloc[states['NY'],:]
TX = mergedDf.iloc[states['TX'],:]
PA = mergedDf.iloc[states['PA'],:]
CA = mergedDf.iloc[states['CA'],:]
MA = mergedDf.iloc[states['MA'],:]
DC = mergedDf.iloc[states['DC'],:]
States = [NY, TX, PA, MA, DC]
no_jobs = [len(x) for x in States]
# Distribution of new jobs by State
plt.bar([0,1,2,3,4],no_jobs)
plt.xticks([0,1,2,3,4],list(states.keys()))
plt.title('Job Distribution by States', fontsize=20)

%time
# Parallel processing
# The scripts are in text_normalize.py
NY_corpus = tn.parallel_normalize_corpus(NY.description.values)
CA_corpus = tn.parallel_normalize_corpus(TX.description.values)
PA_corpus = tn.parallel_normalize_corpus(PA.description.values)


corpuses = {'NY':NY_corpus, 'DC':DC_corpus, 'CA':CA_corpus}
norm_corpus = {}
# Tokernize text corpus
for state, corpus in corpuses.items():
     norm_corpus[state] = [x.split(' ') for x in corpus]

print(len(norm_corpus))
print(norm_corpus['CA'][0][0:50])

# Feature Engineering
cv = CountVectorizer(min_df=5, max_df=0.6, ngram_range=(1,2),
                    token_pattern=None, tokenizer=lambda doc: doc,
                    preprocessor=lambda doc: doc)

cv_features = cv.fit_transform(norm_corpus['NY'])
cv_features.shape

vocabulary = np.array(cv.get_feature_names())
print('Total Vocabulary Size:', len(vocabulary))
Total_Topic = 6
nmf_model = NMF(n_components=Total_Topic, solver='cd', max_iter=500,
               random_state=42, alpha=.1, l1_ratio=.85)

document_topics = nmf_model.fit_transform(cv_features)

topic_terms = nmf_model.components_
topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:,:20]
topic_keyterms = vocabulary[topic_key_term_idxs]
topics = [', '.join(topic) for topic in topic_keyterms]
pd.set_option('display.max_colwidth', -1)
topics_df = pd.DataFrame(topics, columns = ['Terms per Topic'], index = ['Topic' + str(t) for t in range(1,Total_Topic+1)])
topics_df

cv = CountVectorizer(min_df=5, max_df=0.6, ngram_range=(1,2),
                    token_pattern=None, tokenizer=lambda doc: doc,
                    preprocessor=lambda doc: doc)

cv_features = cv.fit_transform(norm_corpus['PA'])
cv_features.shape

vocabulary = np.array(cv.get_feature_names())
print('Total Vocabulary Size:', len(vocabulary))
Total_Topic = 6
nmf_model = NMF(n_components=Total_Topic, solver='cd', max_iter=500,
               random_state=42, alpha=.1, l1_ratio=.85)

document_topics = nmf_model.fit_transform(cv_features)

topic_terms = nmf_model.components_
topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:,:20]
topic_keyterms = vocabulary[topic_key_term_idxs]
topics = [', '.join(topic) for topic in topic_keyterms]
pd.set_option('display.max_colwidth', -1)
topics_df = pd.DataFrame(topics, columns = ['Terms per Topic'], index = ['Topic' + str(t) for t in range(1,Total_Topic+1)])
topics_df

# 
cv = CountVectorizer(min_df=5, max_df=0.6, ngram_range=(1,2),
                    token_pattern=None, tokenizer=lambda doc: doc,
                    preprocessor=lambda doc: doc)

cv_features = cv.fit_transform(norm_corpus['CA'])
cv_features.shape

vocabulary = np.array(cv.get_feature_names())
print('Total Vocabulary Size:', len(vocabulary))
Total_Topic = 6
nmf_model = NMF(n_components=Total_Topic, solver='cd', max_iter=500,
               random_state=42, alpha=.1, l1_ratio=.85)

document_topics = nmf_model.fit_transform(cv_features)

topic_terms = nmf_model.components_
topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:,:20]
topic_keyterms = vocabulary[topic_key_term_idxs]
topics = [', '.join(topic) for topic in topic_keyterms]
pd.set_option('display.max_colwidth', -1)
topics_df = pd.DataFrame(topics, columns = ['Terms per Topic'], index = ['Topic' + str(t) for t in range(1,Total_Topic+1)])
topics_df
